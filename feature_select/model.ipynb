{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('ml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ad5ad074276989c7bb430cb03009529396ebc1f412808063235dd1e7fed6dc18"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import catboost as cab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from multiprocessing import Process\n",
    "\n",
    "# from featexp import get_univariate_plots#用于特征筛选，需要先安装featexp\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['Simhei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "if not os.path.exists('../feature_importance/'):\n",
    "    os.mkdir('../feature_importance/')\n",
    "\n",
    "if not os.path.exists('../prediction/'):\n",
    "    os.mkdir('../prediction/')\n",
    "\n",
    "\n",
    "def eval_score(y_test, y_pre):\n",
    "    _, _, f_class, _ = precision_recall_fscore_support(\n",
    "        y_true=y_test, y_pred=y_pre, labels=[0, 1], average=None)\n",
    "    fper_class = {'合法': f_class[0], '违法': f_class[1],\n",
    "                  'f1': f1_score(y_test, y_pre)}\n",
    "    return fper_class\n",
    "\n",
    "\n",
    "def k_fold_serachParmaters(model, train_val_data, train_val_kind):\n",
    "    mean_f1 = 0\n",
    "    mean_f1Train = 0\n",
    "    n_splits = 5\n",
    "    sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "    for train, test in sk.split(train_val_data, train_val_kind):\n",
    "        x_train = train_val_data.iloc[train]\n",
    "        y_train = train_val_kind.iloc[train]\n",
    "        x_valid = train_val_data.iloc[test]\n",
    "        y_valid = train_val_kind.iloc[test]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_valid)\n",
    "        fper_class = eval_score(y_valid, pred)\n",
    "        mean_f1 += fper_class['f1'] / n_splits\n",
    "\n",
    "        pred_Train = model.predict(x_train)\n",
    "        fper_class_train = eval_score(y_train, pred_Train)\n",
    "        mean_f1Train += fper_class_train['f1'] / n_splits\n",
    "    return mean_f1\n",
    "\n",
    "\n",
    "def cat_model(iter_cnt, lr, max_depth, cat_features):\n",
    "    clf = cab.CatBoostClassifier(iterations=iter_cnt,\n",
    "                                 learning_rate=lr,\n",
    "                                 depth=max_depth,\n",
    "                                 silent=True,\n",
    "                                 thread_count=8,\n",
    "                                 task_type='CPU',\n",
    "                                 cat_features=cat_features\n",
    "                                 )\n",
    "    return clf\n",
    "\n",
    "\n",
    "def cat_serachParm(train_data, train_labels, cat_features):\n",
    "    print('cat_serachParm 搜索最佳参数 .......')\n",
    "    # 搜索最佳参数\n",
    "    param = []\n",
    "    best = 0\n",
    "    best_model = None\n",
    "    #\n",
    "    # 55, 60, 70, 80\n",
    "    for iter_cnt in [55, 60, 70, 80]:\n",
    "        print('iter_cnt:', iter_cnt)\n",
    "        for lr in [0.03, 0.035, 0.040, 0.045, 0.050, 0.055, 0.060, 0.065]:\n",
    "            for max_depth in [5, 6, 7, 8]:\n",
    "                clf = cat_model(iter_cnt, lr, max_depth, cat_features)\n",
    "                mean_f1 = k_fold_serachParmaters(clf,\n",
    "                                                 train_data, train_labels)\n",
    "                if mean_f1 > best:\n",
    "                    param = [iter_cnt, lr, max_depth]\n",
    "                    best = mean_f1\n",
    "                    best_model = clf\n",
    "                    print(param, best)\n",
    "    print('cat_serachParm 搜索最佳参数 ....... over', param, best)\n",
    "    return best_model, param, best\n",
    "\n",
    "\n",
    "def rf_model(n_estimators, max_depth, min_samples_split):\n",
    "    rf = RandomForestClassifier(oob_score=True, random_state=2020,\n",
    "                                n_estimators=n_estimators,\n",
    "                                max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split)\n",
    "    return rf\n",
    "\n",
    "\n",
    "def rf_searchParam(train_data, train_labels):\n",
    "    print('rf_searchParam 搜索最佳参数 .......')\n",
    "    # 搜索最佳参数\n",
    "    param = []\n",
    "    best = 0\n",
    "    best_model = None\n",
    "    for n_estimators in [50, 55, 57, 60, 65]:\n",
    "        print('n_estimators:', n_estimators)\n",
    "        for min_samples_split in [6, 8, 10, 13, 15, 17, 20]:\n",
    "            for max_depth in [11, 12, 13, 15]:\n",
    "                rf = rf_model(n_estimators, max_depth, min_samples_split)\n",
    "                mean_f1 = k_fold_serachParmaters(\n",
    "                    rf, train_data, train_labels)\n",
    "                if mean_f1 > best:\n",
    "                    param = [n_estimators, min_samples_split, max_depth]\n",
    "                    best = mean_f1\n",
    "                    best_model = rf\n",
    "                    print(param, best)\n",
    "    print('rf_searchParam 搜索最佳参数 ....... over', param, best)\n",
    "    return best_model, param, best\n",
    "\n",
    "\n",
    "def lgb_model(n_estimators, max_depth, num_leaves, learning_rate):\n",
    "    lgb_model = lgb.LGBMClassifier(objective='binary',\n",
    "                                   boosting_type='gbdt',\n",
    "                                   tree_learner='serial',\n",
    "                                   num_leaves=num_leaves,\n",
    "                                   max_depth=max_depth,\n",
    "                                   learning_rate=learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   subsample=0.8,\n",
    "                                   feature_fraction=0.8,\n",
    "                                   reg_alpha=0.3,\n",
    "                                   reg_lambda=0.5,\n",
    "                                   random_state=2020,\n",
    "                                   is_unbalance=True)\n",
    "    return lgb_model\n",
    "\n",
    "\n",
    "def lgb_searchParam(train_data, train_labels):\n",
    "    print('lgb_searchParam 搜索最佳参数 .......')\n",
    "    # 搜索最佳参数\n",
    "    param = []\n",
    "    best = 0\n",
    "    best_model = None\n",
    "    # 40, 45, 50, 55, 60, 65, 70\n",
    "    for n_estimators in [40, 45, 50, 55, 60, 65, 70]:\n",
    "        print('n_estimators:', n_estimators)\n",
    "        for max_depth in [6, 7, 8]:\n",
    "            for num_leaves in [40, 45, 50, 55, 60, 65, 70]:\n",
    "                for learning_rate in [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2, 0.25]:\n",
    "                    lgb_cl = lgb_model(n_estimators, max_depth,\n",
    "                                       num_leaves, learning_rate)\n",
    "                    mean_f1 = k_fold_serachParmaters(\n",
    "                        lgb_cl, train_data, train_labels)\n",
    "                    if mean_f1 > best:\n",
    "                        param = [n_estimators, max_depth,\n",
    "                                 num_leaves, learning_rate]\n",
    "                        best = mean_f1\n",
    "                        best_model = lgb_cl\n",
    "                        print(param, best)\n",
    "    print('lgb_searchParam 搜索最佳参数 ....... over', param, best)\n",
    "    return best_model, param, best\n",
    "\n",
    "\n",
    "def predict(name, model, train_data, train_label, test_data, merge=False, n_splits=5, shuffle=True, random_state=2020):\n",
    "    mean_f1 = 0\n",
    "    answers = []\n",
    "    feature_importance_list = []\n",
    "    merge_name = ''\n",
    "    test = test_data.drop('id', axis=1)\n",
    "    if merge:\n",
    "        merge_name = 'merge'\n",
    "    sk = StratifiedKFold(n_splits=n_splits,\n",
    "                         shuffle=True, random_state=2020)\n",
    "    for i, (train, valid) in enumerate(sk.split(train_data, train_label)):\n",
    "        x_train = train_data.iloc[train]\n",
    "        y_train = train_label.iloc[train]\n",
    "        x_valid = train_data.iloc[valid]\n",
    "        y_valid = train_label.iloc[valid]\n",
    "        model.fit(x_train, y_train)\n",
    "        pred_cab = model.predict(x_valid)\n",
    "        f1_score_ = eval_score(y_valid, pred_cab)['f1']\n",
    "        print('model = {} 第{}次验证的f1:{}'.format(model, i + 1, f1_score_))\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'column': train_data.columns,\n",
    "            'importance': model.feature_importances_,\n",
    "        })\n",
    "        feature_importance_list.append(feature_importance)\n",
    "        mean_f1 += f1_score_ / n_splits\n",
    "        ans = model.predict_proba(test)\n",
    "        answers.append(ans)\n",
    "    print('mean f1:', mean_f1)\n",
    "    feature_importances = pd.concat(feature_importance_list)\n",
    "    feature_importances = feature_importances.groupby(\n",
    "        'column')['importance'].agg('mean').sort_values(ascending=False).reset_index()\n",
    "    feature_importances.to_csv(\n",
    "        f'../feature_importance/{name}_{merge_name}_feature_importance.csv')\n",
    "    prediction = np.sqrt(sum(np.array(answers)**2) / n_splits)[:, 1]\n",
    "    predict_res = pd.DataFrame(\n",
    "        {'id': test_data['id'],\n",
    "         \"score\": prediction}\n",
    "    )\n",
    "    predict_res.to_csv(f'../prediction/{name}_{merge_name}_prediction.csv')\n",
    "    return prediction, feature_importances\n",
    "\n",
    "\n",
    "def get_fearures():\n",
    "    feature = pd.read_csv(\n",
    "        '../feature.csv')\n",
    "    entprise_info = pd.read_csv('../data/train/entprise_info.csv')\n",
    "    data = pd.merge(feature, entprise_info, how='left', on='id')\n",
    "    cat_features = ['oplocdistrict', 'industryphy', 'industryco', 'enttype', 'enttypeitem',\n",
    "                    'state', 'orgid', 'jobid', 'adbusign', 'townsign', 'regtype', 'compform', 'opform', 'venind', 'oploc',  'enttypegb', 'industryphy_industryco',\n",
    "                    'enttypegb_enttypeitem', 'nan_num_bin', 'regcap_bin', 'empnum_bin'\n",
    "                    ]\n",
    "    data[cat_features].astype(int)\n",
    "    # print(data.max())\n",
    "    train = data[data.label.notna()]\n",
    "    test = data[data.label.isnull()]\n",
    "\n",
    "    train_data, train_labels = train.drop(\n",
    "        ['id', 'label'], axis=1), train['label']\n",
    "    test_data = test.drop(\n",
    "        ['label'], axis=1)\n",
    "\n",
    "    return train_data, train_labels, test_data, cat_features\n",
    "\n",
    "\n",
    "def get_topK_features(feature_importance_list, k=20):\n",
    "    topk_feature_names = []\n",
    "    for feature_importance in feature_importance_list:\n",
    "        topk_feature_names.extend(\n",
    "            list(feature_importance['column'].values[:20]))\n",
    "    topk_feature_names = set(topk_feature_names)\n",
    "    return topk_feature_names\n",
    "\n",
    "\n",
    "def train(train_data, train_labels, test_data, cat_features, merge=False):\n",
    "    merge_name = ''\n",
    "    if merge:\n",
    "        merge_name = 'merge'\n",
    "\n",
    "    best_cat = cat_serachParm(train_data, train_labels, cat_features)\n",
    "\n",
    "    cat_score, cat_feature_importances = predict('cat',\n",
    "                                                 best_cat, train_data, train_labels, test_data, merge=merge)\n",
    "    best_rf = rf_searchParam(train_data, train_labels)\n",
    "    rf_score, rf_feature_importances = predict('rf',\n",
    "                                               best_rf, train_data, train_labels, test_data, merge=merge)\n",
    "    best_lgb = lgb_searchParam(train_data, train_labels)\n",
    "    lgb_score, lgb_feature_importances = predict('lgb',\n",
    "                                                 best_lgb, train_data, train_labels, test_data, merge=merge)\n",
    "\n",
    "    final_score = (rf_score + cat_score + lgb_score) / 3.0\n",
    "    test_data['score'] = final_score  # 可选:fina_persudo是伪标签的预测结果\n",
    "    submit_csv = test_data[['id', 'score']]\n",
    "    submit_csv.to_csv(f'../submit_{merge_name}.csv', index=False)\n",
    "    return cat_feature_importances, rf_feature_importances, lgb_feature_importances\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data, train_labels, test_data, cat_features = get_fearures()\n",
    "    cat_feature_importances, rf_feature_importances, lgb_feature_importances = train(\n",
    "        train_data, train_labels, test_data, cat_features)\n",
    "    topk_feature_names = get_topK_features(\n",
    "        [cat_feature_importances, rf_feature_importances, lgb_feature_importances])\n",
    "    train_data = train_data[topk_feature_names]\n",
    "    new_cat_features = topk_feature_names.intersection(cat_features)\n",
    "    topk_feature_names.add('id')\n",
    "    train(train_data,\n",
    "          train_labels, test_data[topk_feature_names], new_cat_features, merge=True)\n",
    "\n",
    "    # %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mean_f1_scores = [1,2,3]\n",
    "scores =[0.3,0.4,0.5]\n",
    "max_id, min_id = np.argmax(mean_f1_scores), np.argmin(mean_f1_scores)\n",
    "final_score = 3 / 6 * scores[max_id] + 1 / 6 * \\\n",
    "        scores[min_id] + 2 / 6 * scores[3 - max_id - min_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.43333333333333335"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}